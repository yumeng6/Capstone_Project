{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ujeiPIoTAe5f"
   },
   "source": [
    "## Capstone Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PeB660mvezrD",
    "outputId": "3624d461-ba67-4d86-bb46-aa6f693513f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in /Users/yumengli/anaconda3/lib/python3.7/site-packages (1.1.1)\r\n"
     ]
    }
   ],
   "source": [
    "# install packages\n",
    "# coding: utf-8\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8ClK8p08ZUNe",
    "outputId": "ac1f617f-4d6d-44f0-c59e-59b23b3d6a9c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yumengli/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download corpus of stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "ZvASBJmGAe5h"
   },
   "outputs": [],
   "source": [
    "# import modules\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import scipy.stats as ss\n",
    "import unidecode\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.special import inv_boxcox\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "id": "x9Wi8sAJAe5q",
    "outputId": "f70840e4-f068-4d57-d683-c11b505a7a43"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = pd.read_csv(\"pet food data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kd4rakpAe5x"
   },
   "source": [
    "There are 684 entries that do not have price data, 548 entries that do not have an ingredients list, 755 entries that do not have a total package size, and 747 entries that do not have a unit package size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "QEWKVbzzAe5z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8754 entries, 0 to 8753\n",
      "Data columns (total 13 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   date                  8754 non-null   object \n",
      " 1   category              8754 non-null   object \n",
      " 2   sub_category          8754 non-null   object \n",
      " 3   product               8754 non-null   object \n",
      " 4   variant               8754 non-null   object \n",
      " 5   description           8754 non-null   object \n",
      " 6   ingredients_stdlist   8206 non-null   object \n",
      " 7   price_usd             8070 non-null   float64\n",
      " 8   company_parent        8754 non-null   object \n",
      " 9   company               8754 non-null   object \n",
      " 10  brand                 8754 non-null   object \n",
      " 11  total_pack_size_ml_g  7999 non-null   float64\n",
      " 12  unit_pack_size_ml_g   8007 non-null   float64\n",
      "dtypes: float64(3), object(10)\n",
      "memory usage: 889.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# get a description of the unprocessed data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "w9HB0tUoAe55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine if there are any duplicate rows\n",
    "np.sum(data.duplicated())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WSgAX3J8ho1n"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.unique of 0                         Mars\n",
       "1         Big Heart Pet Brands\n",
       "2                         Iams\n",
       "3                         Iams\n",
       "4       Del Monte Pet Products\n",
       "                 ...          \n",
       "8749                    Target\n",
       "8750     Nestlé Purina PetCare\n",
       "8751      Big Heart Pet Brands\n",
       "8752     Nestlé Purina PetCare\n",
       "8753      Big Heart Pet Brands\n",
       "Name: company, Length: 8754, dtype: object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.company.unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibuuloAjbNBr"
   },
   "source": [
    "### Initial Cleaning for Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4lxLmVfRAe5_"
   },
   "outputs": [],
   "source": [
    "# drop all of the NaN entries\n",
    "data.dropna(inplace=True)\n",
    "\n",
    "# remove 1 outlier price with invalid pack size\n",
    "data = data[data.price_usd < 50]\n",
    "\n",
    "# remove 1 unit pack size equal to 0\n",
    "data = data[data.unit_pack_size_ml_g > 0]\n",
    "\n",
    "# drop 32 therapeutic supplements \n",
    "data = data.loc[~data.ingredients_stdlist.str.startswith(\"active\"), :]\n",
    "\n",
    "# reset the index\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tUiJxAdTAe6E"
   },
   "outputs": [],
   "source": [
    "# transform the string dates into datetime objects and create a year column\n",
    "data.loc[:, \"date\"] = pd.to_datetime(data.date.str[:10])\n",
    "data[\"year\"] = data.date.apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZM9tCy_fAe6I"
   },
   "outputs": [],
   "source": [
    "# drop the \"category\" column since it contains one value \"Pet Food\" and\n",
    "# the \"variant\" column since it is equivalent to the \"product\" column\n",
    "data.drop(columns=[\"category\", \"product\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "8v_t35EX6ReB"
   },
   "outputs": [],
   "source": [
    "# add a ratio column of pack sizes to the dataframe\n",
    "data[\"ratio\"] = data.total_pack_size_ml_g / data.unit_pack_size_ml_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "SLEgvzdBAe6M"
   },
   "outputs": [],
   "source": [
    "# transform the \"sub_category\" column into separate columns and drop \"sub_category\"\n",
    "data[\"pet_type\"] = data.sub_category.apply(lambda x: \"cat\" if (\"Cat\" in x) else \"dog\")\n",
    "data[\"meal_type\"] = data.sub_category.apply(lambda x: \"primary\" if (\"Food\" in x) else \"treats\")\n",
    "data[\"food_type\"] = data.sub_category.apply(lambda x: \"dry\" if (\"Dry\" in x) else \"wet\")\n",
    "data.drop(columns=\"sub_category\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "u2Vauq7JAmp8"
   },
   "outputs": [],
   "source": [
    "# create a function to replace dicritic characters with ASCII characters\n",
    "def replace_dicritic(string):\n",
    "    return unidecode.unidecode(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MRQBb4TJAe6P"
   },
   "outputs": [],
   "source": [
    "# replace the dicritic characters in \"company_parent\", \"company\", \"brand\",\n",
    "# \"description\", and \"ingredients_stdlist\"\n",
    "data.loc[:, \"company_parent\"] = data.company_parent.apply(replace_dicritic)\n",
    "data.loc[:, \"company\"] = data.company.apply(replace_dicritic)\n",
    "data.loc[:, \"brand\"] = data.brand.apply(replace_dicritic)\n",
    "data.loc[:, \"description\"] = data.description.apply(replace_dicritic)\n",
    "data.loc[:, \"ingredients_stdlist\"] = data.ingredients_stdlist.apply(replace_dicritic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "bNZrowcaBE-e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7525 entries, 0 to 7524\n",
      "Data columns (total 15 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   date                  7525 non-null   datetime64[ns]\n",
      " 1   variant               7525 non-null   object        \n",
      " 2   description           7525 non-null   object        \n",
      " 3   ingredients_stdlist   7525 non-null   object        \n",
      " 4   price_usd             7525 non-null   float64       \n",
      " 5   company_parent        7525 non-null   object        \n",
      " 6   company               7525 non-null   object        \n",
      " 7   brand                 7525 non-null   object        \n",
      " 8   total_pack_size_ml_g  7525 non-null   float64       \n",
      " 9   unit_pack_size_ml_g   7525 non-null   float64       \n",
      " 10  year                  7525 non-null   int64         \n",
      " 11  ratio                 7525 non-null   float64       \n",
      " 12  pet_type              7525 non-null   object        \n",
      " 13  meal_type             7525 non-null   object        \n",
      " 14  food_type             7525 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(4), int64(1), object(9)\n",
      "memory usage: 882.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# get a description of the cleaned data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jUs-J-Sfjt-F"
   },
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "nRNzNFwsj3Cv"
   },
   "outputs": [],
   "source": [
    "# get the observations for Colgate only\n",
    "colgate_only = data[data.company_parent.apply(lambda x: \"colgate\" in x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "vdAM5r64eOtw"
   },
   "outputs": [],
   "source": [
    "# perform a Box-Cox transformation on price_usd\n",
    "box_cox = ss.boxcox(data.price_usd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQrLcikrcEMg"
   },
   "source": [
    "Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "G7ejtR3lcKHQ"
   },
   "outputs": [],
   "source": [
    "# boxplot for cat vs dog\n",
    "fig1 = px.box(data, x=\"pet_type\", y=\"price_usd\")\n",
    "fig2 = px.box(colgate_only, x=\"pet_type\", y=\"price_usd\", category_orders={\"pet_type\": [\"cat\", \"dog\"]})\n",
    "\n",
    "# boxplot for primary food vs treats\n",
    "fig3 = px.box(data, x=\"meal_type\", y=\"price_usd\")\n",
    "fig4 = px.box(colgate_only, x=\"meal_type\", y=\"price_usd\")\n",
    "\n",
    "# boxplot for dry vs wet food\n",
    "fig5 = px.box(data, x=\"food_type\", y=\"price_usd\")\n",
    "fig6 = px.box(colgate_only, x=\"food_type\", y=\"price_usd\", category_orders={\"food_type\": [\"dry\", \"wet\"]})\n",
    "\n",
    "# boxplot for year\n",
    "fig7 = px.box(data, x=\"year\", y=\"price_usd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9bwKL178dXim"
   },
   "source": [
    "Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "yN8o-R8Vdb8z"
   },
   "outputs": [],
   "source": [
    "# histogram of price\n",
    "fig8 = px.histogram(data.price_usd)\n",
    "\n",
    "# histogram of price with Box-Cox transformation\n",
    "fig9 = px.histogram(box_cox[0], nbins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCpef8Uag0uL"
   },
   "source": [
    "Scatter Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c7-N1v29g4Aa"
   },
   "outputs": [],
   "source": [
    "# scatter plot of price vs unit pack size\n",
    "fig10 = px.scatter(data.loc[~data.unit_pack_size_ml_g.isna(), :], x=\"unit_pack_size_ml_g\", y=\"price_usd\", trendline=\"ols\", trendline_color_override=\"orange\", range_y=[-2, 37])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRTPUHIchSMZ"
   },
   "source": [
    "### Text Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "awi77x_HhYFa"
   },
   "outputs": [],
   "source": [
    "# get the first word in every ingredient list\n",
    "first_ingredient = data.ingredients_stdlist.str.split(\",\").apply(lambda x: x[0])\n",
    "first_ingredient = first_ingredient.str.split(\" \").apply(lambda x: x[0])\n",
    "first_ingredient = first_ingredient.str.split(\":\").apply(lambda x: x[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "poey8hLaWKds"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chicken    1871\n",
       "wheat       675\n",
       "beef        568\n",
       "corn        412\n",
       "water       350\n",
       "turkey      289\n",
       "salmon      249\n",
       "lamb        225\n",
       "rice        203\n",
       "duck        115\n",
       "Name: ingredients_stdlist, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look at the top 10 words by frequency count\n",
    "first_ingredient.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "fxUIqlyxWM_5"
   },
   "outputs": [],
   "source": [
    "# create a function that filters the first_ingredient Series to\n",
    "# 5 possible words\n",
    "def first_word(string):\n",
    "    if string == \"chicken\" or string == \"wheat\" or string == \"beef\" or string == \"corn\":\n",
    "        return string\n",
    "    else:\n",
    "        return \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "20_brQ9_WPsD"
   },
   "outputs": [],
   "source": [
    "# add a first_ingredient column to the dataset\n",
    "data[\"first_ingredient\"] =  first_ingredient.apply(first_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H_P0d7VZWS3b"
   },
   "source": [
    "TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJXpHBM7sUrf"
   },
   "source": [
    "During testing, we discovered that using a TFIDF matrix with more than ~400 features in our regression models caused instability, resulting in errors that were $10^6$ or greater. To improve performance, we set max_features=400 in our TfidfVectorizer()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "VlhU0GlmWVl3"
   },
   "outputs": [],
   "source": [
    "# instantiate the TFIDF vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords.words(\"english\"), max_features=400)\n",
    "\n",
    "# transform the product descriptions in a TFIDF matrix\n",
    "vectors = vectorizer.fit_transform(data.description)\n",
    "\n",
    "# convert the TFIDF matrix into a dataframe\n",
    "tfidf = pd.DataFrame(vectors.todense().tolist(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "# manually remove any remaining stop words\n",
    "tfidf.drop(columns=[\"10\", \"100\", \"12\", \"13\", \"14\", \"15\", \"16\", \"20\", \"24\", \"25\", \"50\", \"also\", \"lb\", \"lbs\", \"oz\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2ss2WrSWeGa"
   },
   "source": [
    "Convert categorical variables to one-hot vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "wrii0QLUWjex"
   },
   "outputs": [],
   "source": [
    "# get all of the categorical variables in one dataframe\n",
    "cat_vars = data[[\"pet_type\", \"meal_type\", \"food_type\", \"first_ingredient\"]]\n",
    "\n",
    "# transform the categorical variables into one-hot vectors\n",
    "one_hot = pd.get_dummies(cat_vars, prefix=\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eggl3mXoXyAp"
   },
   "source": [
    "### Combine all of the engineered features and create training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "P6EYeGLs60I2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total_pack_size_ml_g</th>\n",
       "      <th>unit_pack_size_ml_g</th>\n",
       "      <th>ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3175.20</td>\n",
       "      <td>3175.20</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10886.40</td>\n",
       "      <td>10886.40</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4445.28</td>\n",
       "      <td>4445.28</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898.88</td>\n",
       "      <td>4898.88</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6123.60</td>\n",
       "      <td>6123.60</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7520</th>\n",
       "      <td>708.75</td>\n",
       "      <td>708.75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7521</th>\n",
       "      <td>708.75</td>\n",
       "      <td>708.75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7522</th>\n",
       "      <td>708.75</td>\n",
       "      <td>708.75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7523</th>\n",
       "      <td>708.75</td>\n",
       "      <td>708.75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>708.75</td>\n",
       "      <td>708.75</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7525 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      total_pack_size_ml_g  unit_pack_size_ml_g  ratio\n",
       "0                  3175.20              3175.20    1.0\n",
       "1                 10886.40             10886.40    1.0\n",
       "2                  4445.28              4445.28    1.0\n",
       "3                  4898.88              4898.88    1.0\n",
       "4                  6123.60              6123.60    1.0\n",
       "...                    ...                  ...    ...\n",
       "7520                708.75               708.75    1.0\n",
       "7521                708.75               708.75    1.0\n",
       "7522                708.75               708.75    1.0\n",
       "7523                708.75               708.75    1.0\n",
       "7524                708.75               708.75    1.0\n",
       "\n",
       "[7525 rows x 3 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gather all of the pack size columns\n",
    "sizes = data[[\"total_pack_size_ml_g\", \"unit_pack_size_ml_g\", \"ratio\"]]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZ1PUCTHho1t"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "kMEY6yCUWycr"
   },
   "outputs": [],
   "source": [
    "# combine the 1hot dataframe with the TFIDF dataframe\n",
    "X = pd.concat([one_hot, sizes, tfidf], axis=1)\n",
    "\n",
    "# create a target variable for the Box-Cox transformed prices\n",
    "y = box_cox[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stratified training and testing sets\n",
    "# box-cox prices\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=data.pet_type, random_state=38)\n",
    "\n",
    "# original prices\n",
    "X_train_org, X_test_org, y_train_org, y_test_org = train_test_split(X, data.price_usd, train_size=0.8, stratify=data.pet_type, random_state=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6020, 399), (1505, 399), (6020,), (1505,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6020, 399), (1505, 399), (6020,), (1505,))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_org.shape, X_test_org.shape, y_train_org.shape, y_test_org.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "r5suSPhjW9sJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7525, 399), (7525,))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Random Forest Regressor using a cross-validated random grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
       " 'max_features': ['auto', 'sqrt'],\n",
       " 'min_samples_leaf': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
       " 'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up a parameter grid for random grid search\n",
    "# max number of levels per tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 100, num = 10)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# min number of samples required at each leaf node\n",
    "min_samples_leaf = [int(x) for x in np.linspace(5, 50, num = 10)]\n",
    "\n",
    "# number of trees in the random forest\n",
    "n_estimators = [int(x) for x in np.linspace(100, 1000, num = 10)]\n",
    "\n",
    "random_grid = {\"max_depth\": max_depth,\n",
    "               \"max_features\": [\"auto\", \"sqrt\"],\n",
    "               \"min_samples_leaf\": min_samples_leaf,\n",
    "               \"n_estimators\": n_estimators}\n",
    "random_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "# instantiate a RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(forest_reg, random_grid,\n",
    "                                   n_iter=100, cv=5,\n",
    "                                   scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# conduct a random search\n",
    "random_search.fit(X_train_org, y_train_org)\n",
    "\n",
    "# view the results of our random search\n",
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted y values\n",
    "ypred = random_search.best_estimator_.predict(X_test_org)\n",
    "\n",
    "# calculate the RMSE of the RandomForestRegressor model\n",
    "rmse = mean_squared_error(y_test_org, ypred, squared=False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Random Forest Regressor using a cross-validated grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a parameter grid for grid search\n",
    "search_grid = {\"max_depth\": [40, 45, 50, 55, 60],\n",
    "               \"min_samples_leaf\": [3],\n",
    "               \"n_estimators\": [700, 750, 800, 850, 900]}\n",
    "search_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a RandomForestRegressor\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "# instantiate a GridSearchCV\n",
    "grid_search = GridSearchCV(forest_reg, search_grid,\n",
    "                             cv=5, n_jobs=6,\n",
    "                             scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# conduct a grid search\n",
    "grid_search.fit(X_train_org, y_train_org)\n",
    "\n",
    "# view the results of our grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted y values\n",
    "ypred = grid_search.best_estimator_.predict(X_test_org)\n",
    "\n",
    "# calculate the RMSE of the RandomForestRegressor model\n",
    "rmse = mean_squared_error(y_test_org, ypred, squared=False)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polynomial regression with Box-Cox transformed prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a custom polynomial regression function\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a parameter grid for grid search\n",
    "search_grid = {\"polynomialfeatures__degree\": np.arange(1, 21),\n",
    "               \"linearregression__fit_intercept\": [True, False],\n",
    "               \"linearregression__normalize\": [True, False]}\n",
    "search_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a GridSearchCV\n",
    "grid_search = GridSearchCV(PolynomialRegression(), search_grid,\n",
    "                           cv=5, n_jobs=6,\n",
    "                           scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# conduct a grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# view the results of our grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted y values\n",
    "ypred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# calculate the RMSE of the RandomForestRegressor model\n",
    "rmse = mean_squared_error(y_test, ypred, squared=False)\n",
    "\n",
    "# reverse the box-cox transformation to get the rmse in dollars\n",
    "inv_boxcox(rmse, box_cox[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the training and testing data\n",
    "scaler = StandardScaler()\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "X_test_std = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge regression with Box-Cox transformed prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a parameter grid for grid search\n",
    "search_grid = {\"alpha\": np.logspace(-6, 4, 11),\n",
    "               \"fit_intercept\": [True, False]}\n",
    "\n",
    "# instantiate a Ridge\n",
    "ridge_reg = Ridge()\n",
    "\n",
    "# instantiate a GridSearch\n",
    "grid_search = GridSearchCV(ridge_reg, search_grid,\n",
    "                           cv=5, n_jobs=6,\n",
    "                           scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# conduct a grid search\n",
    "grid_search.fit(X_train_std, y_train)\n",
    "\n",
    "# view the results of our grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted y values\n",
    "ypred = grid_search.best_estimator_.predict(X_test_std)\n",
    "\n",
    "# calculate the RMSE of the RandomForestRegressor model\n",
    "rmse = mean_squared_error(y_test, ypred, squared=False)\n",
    "\n",
    "# reverse the box-cox transformation to get the rmse in dollars\n",
    "inv_boxcox(rmse, box_cox[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LASSO regression with Box-Cox transformed prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a parameter grid for grid search\n",
    "search_grid = {\"alpha\": np.logspace(-3, -1, 100)}\n",
    "               #\"fit_intercept\": [True, False]}\n",
    "\n",
    "# instantiate a Lasso\n",
    "lasso_reg = Lasso()\n",
    "\n",
    "# instantiate a GridSearch\n",
    "grid_search = GridSearchCV(lasso_reg, search_grid,\n",
    "                           cv=5, n_jobs=6,\n",
    "                           scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# conduct a grid search\n",
    "grid_search.fit(X_train_std, y_train)\n",
    "\n",
    "# view the results of our grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted y values\n",
    "ypred = grid_search.best_estimator_.predict(X_test_std)\n",
    "\n",
    "# calculate the RMSE of the RandomForestRegressor model\n",
    "rmse = mean_squared_error(y_test, ypred, squared=False)\n",
    "\n",
    "# reverse the box-cox transformation to get the rmse in dollars\n",
    "inv_boxcox(rmse, box_cox[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet regression with Box-Cox transformed prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a parameter grid for grid search\n",
    "search_grid = {\"alpha\": np.logspace(-6, 4, 11),\n",
    "               \"l1_ratio\": np.linspace(0.2, 0.8, 4)}\n",
    "\n",
    "# instantiate a ElasticNet\n",
    "elastic_reg = ElasticNet()\n",
    "\n",
    "# instantiate a GridSearch\n",
    "grid_search = GridSearchCV(elastic_reg, search_grid,\n",
    "                           cv=5, n_jobs=6,\n",
    "                           scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "# conduct a grid search\n",
    "grid_search.fit(X_train_std, y_train)\n",
    "\n",
    "# view the results of our grid search\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the predicted y values\n",
    "ypred = grid_search.best_estimator_.predict(X_test_std)\n",
    "\n",
    "# calculate the RMSE of the RandomForestRegressor model\n",
    "rmse = mean_squared_error(y_test, ypred, squared=False)\n",
    "\n",
    "# reverse the box-cox transformation to get the rmse in dollars\n",
    "inv_boxcox(rmse, box_cox[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LRmdzS0YC51"
   },
   "source": [
    "### Predictive Price Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6px9i71oucf-"
   },
   "source": [
    "Optimal Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oct0leMr7jJJ"
   },
   "outputs": [],
   "source": [
    "# standardize the dataset\n",
    "scaler = StandardScaler()\n",
    "X_std = scaler.fit_transform(X)\n",
    "X_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khMv1Vwm8CA1"
   },
   "outputs": [],
   "source": [
    "# instantiate a LASSO regressor with optimal hyperparameters\n",
    "lasso_reg = Lasso(alpha=0.006428)\n",
    "\n",
    "# train the LASSO regressor on the full dataset\n",
    "lasso_reg.fit(X_std, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5BEdzUB9m2J"
   },
   "outputs": [],
   "source": [
    "# get the predicted prices\n",
    "pred = lasso_reg.predict(X_std)\n",
    "\n",
    "# reverse the box-cox transformation to get the predicted prices in dollars\n",
    "pred_usd = inv_boxcox(pred, box_cox[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WFKTBN7O9GuO"
   },
   "source": [
    "Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XvW4WGU59JVt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_murxL_9L6v"
   },
   "outputs": [],
   "source": [
    "# put the coefficients in a Series with their respective names\n",
    "coef = pd.Series(lasso_reg.coef_, index=X.columns.values)\n",
    "\n",
    "# sort the values in place, descending\n",
    "coef.sort_values(ascending=False, inplace=True)\n",
    "\n",
    "# get the top 20 and bottom 20\n",
    "top20_bottom20 = pd.concat([coef[:20], coef[-20:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hq_NXzNn9bCI"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 12))\n",
    "sns.barplot(top20_bottom20.values, top20_bottom20.index.values,\n",
    "            orient=\"h\", palette='coolwarm')\n",
    "plt.xlim(-0.5, 0.7)\n",
    "plt.xticks(fontsize=13)\n",
    "plt.yticks(fontsize=13)\n",
    "plt.title(\"Top 20 Largest and Bottom 20 Smallest Coefficients\", fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LBkCT-IP9c-X"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(np.arange(0, 7525), pred_usd, alpha=0.5, label=\"Predicted\")\n",
    "plt.scatter(np.arange(0, 7525), data.price_usd, alpha=0.25, label=\"Actual\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yMdc9Bpho1v"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BE6lrJwTho1v"
   },
   "source": [
    "## Grouping  Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59lJmTonho1v"
   },
   "source": [
    "### PCA Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7yr7tRfQho1v"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X_std)\n",
    "PCA(n_components=2)\n",
    "principalComponents = pca.fit_transform(X_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5tCC8aIho1v"
   },
   "outputs": [],
   "source": [
    "principalDF = pd.DataFrame(data = principalComponents, columns = ['principal component 1', 'principal component 2'] )\n",
    "principalDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPUN0hQLho1w"
   },
   "source": [
    "PCA for meal type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-cdfQVNho1w"
   },
   "outputs": [],
   "source": [
    "finalDF_mealType = pd.concat([principalDF, data[[\"meal_type\"]]], axis = 1)\n",
    "finalDF_mealType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9W3nqANJho1w"
   },
   "outputs": [],
   "source": [
    "# visualizing PCA\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [\"treats\", \"primary\"]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets, colors):\n",
    "    indx = finalDF_mealType['meal_type'] == target\n",
    "    ax.scatter(finalDF_mealType.loc[indx, 'principal component 1'],\n",
    "              finalDF_mealType.loc[indx, 'principal component 2'],\n",
    "             c = color, s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA for foodtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bH6Zvazho1w"
   },
   "outputs": [],
   "source": [
    "finalDF_foodType = pd.concat([principalDF, data[[\"food_type\"]]], axis = 1)\n",
    "finalDF_foodType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3n8p0o6Nho10"
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.set_xlabel('Principal Component 1', fontsize = 15)\n",
    "ax.set_ylabel('Principal Component 2', fontsize = 15)\n",
    "ax.set_title('2 component PCA', fontsize = 20)\n",
    "targets = [\"dry\", \"wet\"]\n",
    "colors = ['r', 'g']\n",
    "for target, color in zip(targets, colors):\n",
    "    indx = finalDF_foodType['food_type'] == target\n",
    "    ax.scatter(finalDF_foodType.loc[indx, 'principal component 1'],\n",
    "              finalDF_foodType.loc[indx, 'principal component 2'],\n",
    "             c = color, s = 50)\n",
    "ax.legend(targets)\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec Analysis Using Cosine Similarity \n",
    "#### this shows how similar a new description document is to the other existing documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import smart_open\n",
    "## Using Gensim package reading in the corpus\n",
    "def read_corpus(set):       \n",
    "    for i, line in enumerate(set):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "corpus = list(read_corpus(data.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the doc to vec model \n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "model.build_vocab(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "String ='Tibbles & Bits brand organic dog food is naturally gluten and grain-free, and provides your pet with the best variety of raw ingredients. Follow us on Facebook to see all of our cookies, chews, and jerky.'\n",
    "tokens = gensim.utils.simple_preprocess(String)\n",
    "inferred_vector = model.infer_vector(tokens)\n",
    "## choose the top 100 most similar documents with biggest to smallest cosine smilarity score\n",
    "sims = model.docvecs.most_similar([inferred_vector], topn=100)\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of the most similar documents from the tuple sims object\n",
    "# first is index second is cosine similarity score\n",
    "inds=[i[0] for i in sims]\n",
    "# create a new dataframe with these new index of top 100 most similar documents\n",
    "data_new = data.loc[inds]\n",
    "data_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5 most similar documents with the given index \n",
    "top5 = data_new[:5]\n",
    "# get the top 5 most similar pet food descriptions\n",
    "print(top5.description)\n",
    "# get the top 5 most similar pet food company\n",
    "print(top5.company)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the summary statistic of the sales price of top 10 most similar documents\n",
    "data_new[:10].price_usd.describe()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Capstone_Project.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
